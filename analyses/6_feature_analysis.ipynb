{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classification Analysis for Olink Data and feature analyses\n",
    "\n",
    "This notebook performs a machine learning analysis on Olink protein expression data using Random Forest classification with selected features (top 50) and analizes most relevant features.\n",
    "\n",
    "## Input:\n",
    "- olink.xlsx: Protein expression data with columns:\n",
    "  * SampleID: Unique sample identifier\n",
    "  * Group: Clinical group classification\n",
    "  * SubGroup: Clinical subgroup\n",
    "  * Strain: Sample strain type\n",
    "  * age at LP: Age at lumbar puncture\n",
    "  * Sex: Patient sex\n",
    "  * [Protein Names]: NPX values for each protein\n",
    "\n",
    "## Anlysis steps\n",
    "\n",
    "1. **Model Training**\n",
    "   - Implement 5-fold cross-validation\n",
    "   - Train models with different feature sets\n",
    "   - Generate performance metrics\n",
    "   - Calculate standard metrics (accuracy, precision, recall, F1, ROC-AUC)\n",
    "   - Generate confusion matrices\n",
    "   - Analyze feature importance using SHAP and Gini\n",
    "\n",
    "2. **Analyse important features (top 10)**\n",
    "   - Plot biomarker levels across diagnostic groups\n",
    "   - Plot biomarkers for sCJD strain differentiation\n",
    "   - Assess prognostic value of top 20 fatures with Cox regression analyses\n",
    "   - Plot KM curves of top 10 prognostic biomarkers\n",
    "\n",
    "## Outputs\n",
    "- Gini Feature importance plots\n",
    "- SHAP analysis visualisations\n",
    "- Violin plots for top 10 proteins\n",
    "- Violin plots for the proteins distinuishing M1 vs V2 strains\n",
    "- Forest plots of univariate and multivariate Cox regression analyses\n",
    "- KM curves for 10 proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import warnings\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import requests\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter \n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, \n",
    "                             accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score)\n",
    "\n",
    "# SHAP for interpretability\n",
    "import shap\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable inline plotting for Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.dirname(os.getcwd()) + '/data'\n",
    "figure_path = os.path.dirname(os.getcwd()) + '/figures/feature_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Olink Data\n",
    "df = pd.read_excel(data_path + '/curated/olink.xlsx')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'Codon 129', 'SampleID', 'Group', 'Strain', 'age at LP', 'Sex',\n",
    "    'onset-LP', 'onset-death', 'LP-death', 'NP_subtype'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Filter out controls\n",
    "df = df[df['SubGroup'] != 'CTRL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for 5 fold cross validation without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['SubGroup'], axis=1)\n",
    "y = df['SubGroup']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_reduced_model(X, y, cv, k=50, plot_confusion=True):\n",
    "    # Store metrics\n",
    "    accuracies, precisions, recalls, f1_scores, roc_aucs = [], [], [], [], []\n",
    "    all_confusion_matrices = np.zeros((len(y.unique()), len(y.unique())))\n",
    "    classes = sorted(y.unique())\n",
    "    \n",
    "    # Create DataFrames to store feature importances per fold\n",
    "    fold_features_df = pd.DataFrame()  # Will store all features and their importances per fold\n",
    "    fold_shap_class_df = []  # Will store SHAP values per class per fold\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(cv.split(X, y), 1):\n",
    "        print(f\"Processing fold {fold}\")\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Feature selection\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "        X_test_selected = selector.transform(X_test)\n",
    "        \n",
    "        # Get selected feature names for this fold\n",
    "        feature_mask = selector.get_support()\n",
    "        fold_features = X.columns[feature_mask].tolist()\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        y_pred_proba = model.predict_proba(X_test_selected)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "        \n",
    "        # ROC AUC\n",
    "        y_test_bin = label_binarize(y_test, classes=classes)\n",
    "        roc_auc = roc_auc_score(y_test_bin, y_pred_proba, average='macro', multi_class='ovr')\n",
    "        roc_aucs.append(roc_auc)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        all_confusion_matrices += cm\n",
    "        \n",
    "        # Store Gini importances for this fold\n",
    "        fold_gini = pd.DataFrame({\n",
    "            'Feature': fold_features,\n",
    "            'Gini_Importance': model.feature_importances_,\n",
    "            'Fold': fold\n",
    "        })\n",
    "        \n",
    "        # SHAP analysis\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer(X_test_selected)\n",
    "        \n",
    "        # Calculate SHAP importance per class for this fold\n",
    "        shap_class_importance = np.abs(shap_values.values).mean(axis=0)  # Average over samples\n",
    "        shap_class_df = pd.DataFrame(\n",
    "            shap_class_importance,\n",
    "            columns=classes,\n",
    "            index=fold_features\n",
    "        )\n",
    "        shap_class_df['Fold'] = fold\n",
    "        fold_shap_class_df.append(shap_class_df)\n",
    "        \n",
    "        # Calculate overall SHAP importance for this fold\n",
    "        overall_shap = np.abs(shap_values.values).mean(axis=0).mean(axis=1)  # Average over samples and classes\n",
    "        fold_gini['SHAP_Importance'] = overall_shap\n",
    "        \n",
    "        # Append to main tracking DataFrame\n",
    "        fold_features_df = pd.concat([fold_features_df, fold_gini])\n",
    "    \n",
    "    # Calculate average importance across folds\n",
    "    # First, get list of all unique features that appeared in any fold\n",
    "    all_features = fold_features_df['Feature'].unique()\n",
    "    \n",
    "    # Calculate average importance for features, counting only folds where they appeared\n",
    "    final_rankings = []\n",
    "    for feature in all_features:\n",
    "        feature_data = fold_features_df[fold_features_df['Feature'] == feature]\n",
    "        n_folds = len(feature_data)\n",
    "        avg_gini = feature_data['Gini_Importance'].mean()\n",
    "        avg_shap = feature_data['SHAP_Importance'].mean()\n",
    "        \n",
    "        final_rankings.append({\n",
    "            'Feature': feature,\n",
    "            'Gini_Importance': avg_gini,\n",
    "            'SHAP_Importance': avg_shap,\n",
    "            'Folds_Appeared': n_folds\n",
    "        })\n",
    "    \n",
    "    # Create final rankings DataFrame\n",
    "    feature_rankings = pd.DataFrame(final_rankings)\n",
    "    feature_rankings.sort_values('SHAP_Importance', ascending=False, inplace=True)\n",
    "    \n",
    "    # Calculate average SHAP importance per class\n",
    "    all_shap_class = pd.concat(fold_shap_class_df)\n",
    "    shap_by_class = all_shap_class.groupby(all_shap_class.index).mean()  # Average across folds\n",
    "    shap_by_class = shap_by_class.drop('Fold', axis=1)\n",
    "    \n",
    "    # Get top 20 features by overall SHAP importance\n",
    "    top_20_features = feature_rankings['Feature'].head(20).tolist()\n",
    "    top_20_shap_by_class = shap_by_class.loc[top_20_features]\n",
    "    \n",
    "    # Save results\n",
    "    feature_rankings.to_csv(data_path + '/results/feature_importance_rankings.csv', index=False)\n",
    "    top_20_shap_by_class.to_csv(data_path + '/results/top_20_shap_importance_by_class.csv')\n",
    "    \n",
    "    # Create visualizations\n",
    "    if plot_confusion:\n",
    "        # Confusion Matrix\n",
    "        all_confusion_matrices = all_confusion_matrices / cv.get_n_splits()\n",
    "        all_confusion_matrices = (all_confusion_matrices / all_confusion_matrices.sum(axis=1, keepdims=True)) * 100\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(all_confusion_matrices, annot=True, fmt='.2f', cmap='Blues',\n",
    "                   xticklabels=classes, yticklabels=classes,\n",
    "                   annot_kws={\"size\": 12})\n",
    "        plt.title('Confusion matrix (%, RF selected features)', fontsize=14)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.savefig(figure_path + '/confusion_matrix_reduced_rf.png', dpi=1200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Gini Importance Plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_20_gini = feature_rankings.head(20)\n",
    "        plt.barh(range(20), top_20_gini['Gini_Importance'])\n",
    "        plt.yticks(range(20), top_20_gini['Feature'])\n",
    "        plt.xlabel('Average Gini Importance')\n",
    "        plt.title('Top 20 Feature Importance (Gini)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figure_path + '/top_20_features_gini_importance.png', dpi=1200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # SHAP Importance Heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        im = plt.imshow(top_20_shap_by_class, cmap='Blues')\n",
    "        colorbar=plt.colorbar(im, label='Average |SHAP value|', orientation='vertical')\n",
    "        colorbar.set_label('Average |SHAP value|', fontsize=12)\n",
    "        plt.xticks(range(len(top_20_shap_by_class.columns)), top_20_shap_by_class.columns, rotation=45)\n",
    "        plt.yticks(range(len(top_20_shap_by_class.index)), top_20_shap_by_class.index, fontsize=12)\n",
    "        plt.title('Top 20 Feature Importance Heatmap (SHAP)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figure_path + '/top_20_shap_importance_heatmap.png', dpi=1200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Feature Importance Stability\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        for feature in top_20_features:\n",
    "            feature_stability = fold_features_df[fold_features_df['Feature'] == feature]\n",
    "            plt.plot(feature_stability['Fold'], feature_stability['Gini_Importance'], \n",
    "                    label=feature, marker='o')\n",
    "        plt.xlabel('Fold')\n",
    "        plt.ylabel('Gini Importance')\n",
    "        plt.title('Top 20 Feature Importance Stability Across Folds')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(figure_path + '/feature_importance_stability.png', dpi=1200, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(f\"Accuracy: {np.mean(accuracies):.4f} (±{np.std(accuracies):.4f})\")\n",
    "    print(f\"Precision: {np.mean(precisions):.4f} (±{np.std(precisions):.4f})\")\n",
    "    print(f\"Recall: {np.mean(recalls):.4f} (±{np.std(recalls):.4f})\")\n",
    "    print(f\"F1 Score: {np.mean(f1_scores):.4f} (±{np.std(f1_scores):.4f})\")\n",
    "    print(f\"ROC-AUC: {np.mean(roc_aucs):.4f} (±{np.std(roc_aucs):.4f})\")\n",
    "    \n",
    "    print(\"\\nTop 20 Features by SHAP Importance:\")\n",
    "    print(\"(showing number of folds each feature appeared in)\")\n",
    "    for _, row in feature_rankings.head(20).iterrows():\n",
    "        print(f\"{row['Feature']}: {row['SHAP_Importance']:.4f} (appeared in {row['Folds_Appeared']}/5 folds)\")\n",
    "    \n",
    "    return {\n",
    "        'metrics': {\n",
    "            'Accuracy': (np.mean(accuracies), np.std(accuracies)),\n",
    "            'Precision': (np.mean(precisions), np.std(precisions)),\n",
    "            'Recall': (np.mean(recalls), np.std(recalls)),\n",
    "            'F1 Score': (np.mean(f1_scores), np.std(f1_scores)),\n",
    "            'ROC-AUC': (np.mean(roc_aucs), np.std(roc_aucs))\n",
    "        },\n",
    "        'feature_rankings': feature_rankings,\n",
    "        'shap_by_class': shap_by_class,\n",
    "        'confusion_matrix': all_confusion_matrices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = train_and_evaluate_reduced_model(X, y, cv, k=50, plot_confusion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_path + '/curated/olink.xlsx')\n",
    "feature_importance_rankings = pd.read_csv(data_path + '/results/feature_importance_rankings.csv')\n",
    "top_biomarkers = list(feature_importance_rankings['Feature'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_info(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch protein information from UniProt API\n",
    "    Returns tuple of (full_name, uniprot_id) if found, otherwise (protein_id, None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query UniProt API\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/search?query={protein_id}&format=json\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if we got any results\n",
    "        if data['results']:\n",
    "            result = data['results'][0]  # Take first match\n",
    "            protein_name = result['proteinDescription']['recommendedName']['fullName']['value']\n",
    "            uniprot_id = result['primaryAccession']\n",
    "            return f\"{protein_name} ({protein_id}, {uniprot_id})\"\n",
    "        return protein_id, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching info for {protein_id}: {e}\")\n",
    "        return protein_id, None\n",
    "\n",
    "protein_names = []\n",
    "for protein in top_biomarkers:\n",
    "    full_name = get_protein_info(protein)\n",
    "    protein_names.append(full_name)\n",
    "\n",
    "# Print results\n",
    "for protein, name in zip(top_biomarkers, protein_names):\n",
    "    print(f\"{protein}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mann_whitney(data, column, group1, group2):\n",
    "    x = data[data['SubGroup'] == group1][column].dropna()\n",
    "    y = data[data['SubGroup'] == group2][column].dropna()\n",
    "    \n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        warnings.warn(f\"Insufficient data for Mann-Whitney U test between {group1} and {group2} for {column}\")\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        statistic, p_value = stats.mannwhitneyu(x, y, alternative='two-sided')\n",
    "        return p_value\n",
    "    except ValueError as e:\n",
    "        warnings.warn(f\"Error performing Mann-Whitney U test between {group1} and {group2} for {column}: {str(e)}\")\n",
    "        return np.nan\n",
    "\n",
    "# Define color scheme\n",
    "colors = {\n",
    "    'MV2K': '#2ecc71',  # green\n",
    "    'VV2': '#e74c3c',   # red\n",
    "    'MM(V)1': '#3498db',   # blue\n",
    "    'CTRL': '#333c42'   # grey\n",
    "}\n",
    "\n",
    "# Create a dictionary to map column names to protein names\n",
    "protein_name_dict = dict(zip(top_biomarkers, protein_names))\n",
    "\n",
    "# Get all unique groups\n",
    "groups = df['SubGroup'].unique()\n",
    "\n",
    "# Generate all pairwise combinations of groups\n",
    "group_pairs = list(combinations(groups, 2))\n",
    "\n",
    "# Perform statistical tests and store results\n",
    "stats_results = []\n",
    "for column in top_biomarkers:\n",
    "    for pair in group_pairs:\n",
    "        p_value = perform_mann_whitney(df, column, pair[0], pair[1])\n",
    "        stats_results.append({\n",
    "            'Column': column,\n",
    "            'Group1': pair[0],\n",
    "            'Group2': pair[1],\n",
    "            'P-value': p_value\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "stats_df = pd.DataFrame(stats_results)\n",
    "\n",
    "# Function to get significance stars\n",
    "def get_significance_stars(p_value):\n",
    "    if pd.isna(p_value):\n",
    "        return 'N/A'\n",
    "    elif p_value <= 0.001:\n",
    "        return '***'\n",
    "    elif p_value <= 0.01:\n",
    "        return '**'\n",
    "    elif p_value <= 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "# Add significance stars to the DataFrame\n",
    "stats_df['Significance'] = stats_df['P-value'].apply(get_significance_stars)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Create a boxplot for each specified column\n",
    "for i, column in enumerate(top_biomarkers, 1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    \n",
    "    # Create violin plot with custom colors\n",
    "    sns.violinplot(x='SubGroup', y=column, data=df, \n",
    "                  palette=colors, alpha=0.3, inner=None)\n",
    "    \n",
    "    # Create box plot with custom colors\n",
    "    sns.boxplot(x='SubGroup', y=column, data=df, \n",
    "                palette=colors, width=0.3, showfliers=False)\n",
    "    \n",
    "    # Add significance annotations\n",
    "    y_max = df[column].max()\n",
    "    y_range = df[column].max() - df[column].min()\n",
    "    \n",
    "    for idx, row in stats_df[stats_df['Column'] == column].iterrows():\n",
    "        x1 = list(groups).index(row['Group1'])\n",
    "        x2 = list(groups).index(row['Group2'])\n",
    "        y = y_max + y_range * 0.05 * (1 + abs(x1 - x2))\n",
    "        plt.plot([x1, x2], [y, y], 'k-', linewidth=1)\n",
    "        plt.text((x1 + x2) / 2, y, row['Significance'], \n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    title = protein_name_dict[column]\n",
    "    split_title = title.replace(\" (\", \"\\n(\")  # Splits at the opening parenthesis\n",
    "    plt.title(split_title, fontsize=10, fontweight='bold')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Relative protein abundance (log2)')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_path + '/top10_all.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize most interesting M1 vs V2 proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_path + '/curated/olink.xlsx')\n",
    "feature_importance_rankings = pd.read_csv(data_path + '/results/feature_importance_rankings.csv')\n",
    "\n",
    "# Select specific proteins\n",
    "top_biomarkers = [\"GPC5\", \"CCDC80\", \"WASF1\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_info(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch protein information from UniProt API\n",
    "    Returns tuple of (full_name, uniprot_id) if found, otherwise (protein_id, None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query UniProt API\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/search?query={protein_id}&format=json\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if we got any results\n",
    "        if data['results']:\n",
    "            result = data['results'][0]  # Take first match\n",
    "            protein_name = result['proteinDescription']['recommendedName']['fullName']['value']\n",
    "            uniprot_id = result['primaryAccession']\n",
    "            return f\"{protein_name} ({protein_id}, {uniprot_id})\"\n",
    "        return protein_id, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching info for {protein_id}: {e}\")\n",
    "        return protein_id, None\n",
    "\n",
    "protein_names = []\n",
    "for protein in top_biomarkers:\n",
    "    full_name = get_protein_info(protein)\n",
    "    protein_names.append(full_name)\n",
    "\n",
    "# Print results\n",
    "for protein, name in zip(top_biomarkers, protein_names):\n",
    "    print(f\"{protein}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mann_whitney(data, column, group1, group2):\n",
    "    x = data[data['SubGroup'] == group1][column].dropna()\n",
    "    y = data[data['SubGroup'] == group2][column].dropna()\n",
    "    \n",
    "    if len(x) < 2 or len(y) < 2:\n",
    "        warnings.warn(f\"Insufficient data for Mann-Whitney U test between {group1} and {group2} for {column}\")\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        statistic, p_value = stats.mannwhitneyu(x, y, alternative='two-sided')\n",
    "        return p_value\n",
    "    except ValueError as e:\n",
    "        warnings.warn(f\"Error performing Mann-Whitney U test between {group1} and {group2} for {column}: {str(e)}\")\n",
    "        return np.nan\n",
    "\n",
    "# Define color scheme\n",
    "colors = {\n",
    "    'MV2K': '#2ecc71',  # green\n",
    "    'VV2': '#e74c3c',   # red\n",
    "    'MM(V)1': '#3498db',   # blue\n",
    "    'CTRL': '#333c42'   # grey\n",
    "}\n",
    "\n",
    "# Create a dictionary to map column names to protein names\n",
    "protein_name_dict = dict(zip(top_biomarkers, protein_names))\n",
    "\n",
    "# Get all unique groups\n",
    "groups = df['SubGroup'].unique()\n",
    "\n",
    "# Generate all pairwise combinations of groups\n",
    "group_pairs = list(combinations(groups, 2))\n",
    "\n",
    "# Perform statistical tests and store results\n",
    "stats_results = []\n",
    "for column in top_biomarkers:\n",
    "    for pair in group_pairs:\n",
    "        p_value = perform_mann_whitney(df, column, pair[0], pair[1])\n",
    "        stats_results.append({\n",
    "            'Column': column,\n",
    "            'Group1': pair[0],\n",
    "            'Group2': pair[1],\n",
    "            'P-value': p_value\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "stats_df = pd.DataFrame(stats_results)\n",
    "\n",
    "# Function to get significance stars\n",
    "def get_significance_stars(p_value):\n",
    "    if pd.isna(p_value):\n",
    "        return 'N/A'\n",
    "    elif p_value <= 0.001:\n",
    "        return '***'\n",
    "    elif p_value <= 0.01:\n",
    "        return '**'\n",
    "    elif p_value <= 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "\n",
    "# Add significance stars to the DataFrame\n",
    "stats_df['Significance'] = stats_df['P-value'].apply(get_significance_stars)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Create a boxplot for each specified column\n",
    "for i, column in enumerate(top_biomarkers, 1):\n",
    "    plt.subplot(2, 5, i)\n",
    "    \n",
    "    # Create violin plot with custom colors\n",
    "    sns.violinplot(x='SubGroup', y=column, data=df, \n",
    "                  palette=colors, alpha=0.3, inner=None)\n",
    "    \n",
    "    # Create box plot with custom colors\n",
    "    sns.boxplot(x='SubGroup', y=column, data=df, \n",
    "                palette=colors, width=0.3, showfliers=False)\n",
    "    \n",
    "    # Add significance annotations\n",
    "    y_max = df[column].max()\n",
    "    y_range = df[column].max() - df[column].min()\n",
    "    \n",
    "    for idx, row in stats_df[stats_df['Column'] == column].iterrows():\n",
    "        x1 = list(groups).index(row['Group1'])\n",
    "        x2 = list(groups).index(row['Group2'])\n",
    "        y = y_max + y_range * 0.05 * (1 + abs(x1 - x2))\n",
    "        plt.plot([x1, x2], [y, y], 'k-', linewidth=1)\n",
    "        plt.text((x1 + x2) / 2, y, row['Significance'], \n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    title = protein_name_dict[column]\n",
    "    split_title = title.replace(\" (\", \"\\n(\")  # Splits at the opening parenthesis\n",
    "    plt.title(split_title, fontsize=12)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Relative protein abundance (log2)')\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_path + '/interesting_bmk_all.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Cox regression analysis for top biomarkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_path + '/curated/olink.xlsx')\n",
    "feature_importance_rankings = pd.read_csv(data_path + '/results/feature_importance_rankings.csv')\n",
    "protein_list = list(feature_importance_rankings['Feature'].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the DataFrame\n",
    "df_cox = df.copy()\n",
    "df_cox = df_cox[df_cox['SubGroup'] != 'CTRL']\n",
    "# Rename columns for clarity\n",
    "df_cox = df_cox.rename(columns={ 'LP-death': 'Survival_time', 'onset-LP': 'Onset_lp', 'age at LP': 'Age'})\n",
    "\n",
    "\n",
    "df_cox['Event'] = 1\n",
    "\n",
    "# Scale age (subtract mean and divide by standard deviation)\n",
    "df_cox['Age_scaled'] = (df_cox['Age'] - df_cox['Age'].mean()) / df_cox['Age'].std()\n",
    "\n",
    "# Scale onset to LP column\n",
    "df_cox['Onset_lp_scaled'] = (df_cox['Onset_lp'] - df_cox['Onset_lp'].mean()) / df_cox['Onset_lp'].std()\n",
    "\n",
    "# Convert the SubGroup to numeric values\n",
    "subgroup_mapping = {\n",
    "    'MV2K': 1,\n",
    "    'VV2': 2,\n",
    "    'MM(V)1': 3\n",
    "}\n",
    "\n",
    "df_cox['SubGroup_numeric'] = df_cox['SubGroup'].map(subgroup_mapping)\n",
    "\n",
    "# Convert the codon 129 to numeric values\n",
    "codon_mapping = {\n",
    "    'MV': 1,\n",
    "    'VV': 2,\n",
    "    'MM': 3\n",
    "}\n",
    "\n",
    "df_cox['codon_numeric'] = df_cox['Codon 129'].map(codon_mapping)\n",
    "\n",
    "# Combine protein list with other columns\n",
    "columns_to_select = protein_list + ['Survival_time', 'Event', \n",
    "                                    'codon_numeric',\n",
    "                                    'Sex', 'Age_scaled', 'Onset_lp_scaled', 'SubGroup_numeric']\n",
    "\n",
    "# Select the columns\n",
    "df_cox = df_cox[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cox analysis with selected covariates (optionally excluding SubGroup_numeric)\n",
    "def run_cox_analysis_with_selected_covariates(data, protein_list, covariates):\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Initialize results dataframe dynamically based on covariates\n",
    "    if covariates:\n",
    "        results_columns = ['Protein', 'Hazard Ratio', 'CI_lower', 'CI_upper', \n",
    "                           'p_value', 'Covariate', 'Covariate_HR', 'Covariate_p_value']\n",
    "    else:\n",
    "        results_columns = ['Protein', 'Hazard Ratio', 'CI_lower', 'CI_upper', 'p_value']\n",
    "\n",
    "    results = pd.DataFrame(columns=results_columns)\n",
    "    \n",
    "    # Base columns for Cox model (excluding the specific covariates if necessary)\n",
    "    if not covariates:  # Univariate case\n",
    "        base_cols = ['Survival_time', 'Event']\n",
    "    else:  # Multivariate case\n",
    "        base_cols = ['Survival_time', 'Event', 'codon_numeric', 'Sex', 'Age_scaled', 'Onset_lp_scaled']\n",
    "        if 'SubGroup_numeric' in covariates:\n",
    "            base_cols.append('SubGroup_numeric')\n",
    "\n",
    "    # Loop through proteins and fit Cox model\n",
    "    for protein in protein_list:\n",
    "        try:\n",
    "            # Prepare data for current protein\n",
    "            cols_to_use = base_cols + [protein]\n",
    "            current_data = data[cols_to_use].dropna()\n",
    "\n",
    "            # Fit the Cox model\n",
    "            cph = CoxPHFitter()\n",
    "            cph.fit(current_data, duration_col='Survival_time', event_col='Event')\n",
    "\n",
    "            # Verify proportional hazard assumption\n",
    "            if covariates:\n",
    "                cph.check_assumptions(current_data, p_value_threshold=0.05, show_plots=False)\n",
    "            \n",
    "            # Get protein results\n",
    "            hr_protein = np.exp(cph.params_[protein])\n",
    "            p_val_protein = cph.summary.loc[protein, 'p']\n",
    "            se_protein = cph.summary.loc[protein, 'se(coef)']\n",
    "            ci_lower = np.exp(cph.params_[protein] - 1.96 * se_protein)\n",
    "            ci_upper = np.exp(cph.params_[protein] + 1.96 * se_protein)\n",
    "            \n",
    "            # Add protein result to the results dataframe\n",
    "            if covariates:\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Protein': [protein],\n",
    "                    'Hazard Ratio': [hr_protein],\n",
    "                    'CI_lower': [ci_lower],\n",
    "                    'CI_upper': [ci_upper],\n",
    "                    'p_value': [p_val_protein],\n",
    "                    'Covariate': ['Protein'],\n",
    "                    'Covariate_HR': [hr_protein],\n",
    "                    'Covariate_p_value': [p_val_protein]\n",
    "                })\n",
    "            else:\n",
    "                new_row = pd.DataFrame({\n",
    "                    'Protein': [protein],\n",
    "                    'Hazard Ratio': [hr_protein],\n",
    "                    'CI_lower': [ci_lower],\n",
    "                    'CI_upper': [ci_upper],\n",
    "                    'p_value': [p_val_protein]\n",
    "                })\n",
    "            \n",
    "            results = pd.concat([results, new_row], ignore_index=True)\n",
    "            \n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Sort by p-value\n",
    "    results = results.sort_values('p_value')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 1. Univariate analysis (only protein)\n",
    "covariates_no_covariates = []  # No other covariates, just the protein itself\n",
    "results_univariate = run_cox_analysis_with_selected_covariates(df_cox, protein_list, covariates_no_covariates)\n",
    "\n",
    "# 2. Analysis with all covariates except SubGroup_numeric\n",
    "covariates_without_subgroup = ['codon_numeric', 'Sex', 'Age_scaled', 'Onset_lp_scaled']\n",
    "results_without_subgroup = run_cox_analysis_with_selected_covariates(df_cox, protein_list, covariates_without_subgroup)\n",
    "\n",
    "# 3. Analysis with all covariates\n",
    "covariates_with_all = ['codon_numeric', 'SubGroup_numeric', 'Sex', 'Age_scaled', 'Onset_lp_scaled']\n",
    "results_with_all_covariates = run_cox_analysis_with_selected_covariates(df_cox, protein_list, covariates_with_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forest_plot(results, ax, title, protein_order):\n",
    "    # Ensure proteins are sorted in the same order as `protein_order`\n",
    "    results['Protein'] = pd.Categorical(results['Protein'], categories=protein_order, ordered=True)\n",
    "    plot_data = results.drop_duplicates(subset=['Protein']).sort_values('Protein')  # Sort by predefined order\n",
    "    y_positions = np.arange(len(plot_data))\n",
    "\n",
    "    def format_pvalue(p):\n",
    "        return 'p < 0.001' if p < 0.001 else f'p = {p:.3f}'\n",
    "\n",
    "    # Add alternating grey background for better readability\n",
    "    for i in range(len(plot_data)):\n",
    "        if i % 2 == 0:\n",
    "            ax.axhspan(i - 0.5, i + 0.5, color='gray', alpha=0.1)\n",
    "\n",
    "    # Scatter plot for hazard ratios\n",
    "    ax.scatter(plot_data['Hazard Ratio'], y_positions, c='#F6A15C', s=50, zorder=2)\n",
    "\n",
    "    # Confidence interval lines\n",
    "    for i, (hr, ci_l, ci_u) in enumerate(zip(plot_data['Hazard Ratio'], plot_data['CI_lower'], plot_data['CI_upper'])):\n",
    "        ax.plot([ci_l, ci_u], [i, i], color='#F6A15C', linewidth=2, zorder=1)\n",
    "\n",
    "    # Reference line at HR = 1\n",
    "    ax.axvline(x=1, color='gray', linestyle='--', alpha=0.7, zorder=0)\n",
    "\n",
    "    # Set Y-ticks and labels for each subplot\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(plot_data['Protein'], fontsize=10)  # Ensure labels appear on all subplots\n",
    "\n",
    "    # Add hazard ratios and p-values as text annotations\n",
    "    max_ci = plot_data['CI_upper'].max()\n",
    "    for i, row in enumerate(plot_data.itertuples()):\n",
    "        hr_text = f'HR = {row._2:.2f} ({row.CI_lower:.2f}-{row.CI_upper:.2f})'\n",
    "        p_text = format_pvalue(row.p_value)\n",
    "        ax.text(max_ci * 1.1, i, f'{hr_text}\\n{p_text}', va='center', fontsize=9)\n",
    "\n",
    "    ax.set_xlabel('Hazard Ratio (95% CI)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "\n",
    "# Get consistent protein order from `results_univariate`\n",
    "protein_order = results_univariate['Protein'].unique()\n",
    "\n",
    "# Create a single figure with three horizontally aligned subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 7))\n",
    "\n",
    "# Generate the three forest plots with consistent protein order\n",
    "create_forest_plot(results_univariate, axes[0], 'Univariate Cox regression', protein_order)\n",
    "create_forest_plot(results_without_subgroup, axes[1], 'Multivariate Cox regression', protein_order)\n",
    "create_forest_plot(results_with_all_covariates, axes[2], 'Multivariate Cox regression including sCJD subtype', protein_order)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)  # Adjust spacing between subplots\n",
    "\n",
    "# Save and show the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(figure_path + '/cox_forest_plots_side_by_side.png', bbox_inches='tight', dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaplan-Meyer curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(data_path + '/curated/olink.xlsx')\n",
    "feature_importance_rankings = pd.read_csv(data_path + '/results/feature_importance_rankings.csv')\n",
    "# Define your specific list of proteins\n",
    "top_biomarkers = ['CCDC80', 'MAPT', 'PRDX3', 'FKBP4', 'APEX1', 'FOSB', 'PAG1', 'PPP3R1', 'THOP1', 'WASF1']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_info(protein_id):\n",
    "    \"\"\"\n",
    "    Fetch protein information from UniProt API\n",
    "    Returns tuple of (full_name, uniprot_id) if found, otherwise (protein_id, None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Query UniProt API\n",
    "        url = f\"https://rest.uniprot.org/uniprotkb/search?query={protein_id}&format=json\"\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if we got any results\n",
    "        if data['results']:\n",
    "            result = data['results'][0]  # Take first match\n",
    "            protein_name = result['proteinDescription']['recommendedName']['fullName']['value']\n",
    "            uniprot_id = result['primaryAccession']\n",
    "            return f\"{protein_name} ({protein_id}, {uniprot_id})\"\n",
    "        return protein_id, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching info for {protein_id}: {e}\")\n",
    "        return protein_id, None\n",
    "\n",
    "protein_names = []\n",
    "for protein in top_biomarkers:\n",
    "    full_name = get_protein_info(protein)\n",
    "    protein_names.append(full_name)\n",
    "\n",
    "# Print results\n",
    "for protein, name in zip(top_biomarkers, protein_names):\n",
    "    print(f\"{protein}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['SubGroup'] != 'CTRL']\n",
    "df = df.rename(columns={\n",
    "    'Age at LP': 'age',\n",
    "    'LP-death': 'survival_time'\n",
    "})\n",
    "\n",
    "# Add event indicator for survival analysis\n",
    "df['event'] = 1\n",
    "df = df[df['survival_time'] > 0]\n",
    "df = df[df['survival_time'] < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiplot_survival_analysis(df, protein_list, protein_names):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    gs = GridSpec(2, 5, figure=fig)\n",
    "    \n",
    "    colors = {\n",
    "        'MV2K': {'Upper': '#1a9850', 'Lower': '#91cf60'},     # Dark and light green\n",
    "        'VV2': {'Upper': '#d73027', 'Lower': '#fc8d59'},      # Dark and light red\n",
    "        'MM(V)1': {'Upper': '#2166ac', 'Lower': '#67a9cf'}       # Dark and light blue\n",
    "    }\n",
    "    \n",
    "    # Process each protein\n",
    "    for idx, (protein_id, protein_name) in enumerate(zip(protein_list, protein_names)):\n",
    "        row = idx // 5\n",
    "        col = idx % 5\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        stats_results = {}\n",
    "        \n",
    "        # Process each subtype separately\n",
    "        for subtype in sorted(df['SubGroup'].unique()):\n",
    "            subtype_data = df[df['SubGroup'] == subtype].copy()\n",
    "            \n",
    "            # Calculate tertiles within this subtype\n",
    "            tertiles = subtype_data[protein_id].quantile([0.33, 0.67])\n",
    "            subtype_data['expression_group'] = pd.qcut(subtype_data[protein_id], \n",
    "                                                     q=3, \n",
    "                                                     labels=['Lower', 'Middle', 'Upper'])\n",
    "            \n",
    "            # Get data for upper and lower tertiles only\n",
    "            plot_data = subtype_data[subtype_data['expression_group'].isin(['Upper', 'Lower'])]\n",
    "            \n",
    "            # Calculate group sizes\n",
    "            upper_n = len(plot_data[plot_data['expression_group'] == 'Upper'])\n",
    "            lower_n = len(plot_data[plot_data['expression_group'] == 'Lower'])\n",
    "            \n",
    "            # Perform logrank test\n",
    "            if upper_n > 0 and lower_n > 0:\n",
    "                results = logrank_test(\n",
    "                    plot_data[plot_data['expression_group'] == 'Upper']['survival_time'],\n",
    "                    plot_data[plot_data['expression_group'] == 'Lower']['survival_time'],\n",
    "                    plot_data[plot_data['expression_group'] == 'Upper']['event'],\n",
    "                    plot_data[plot_data['expression_group'] == 'Lower']['event']\n",
    "                )\n",
    "                \n",
    "                stats_results[subtype] = {\n",
    "                    'p_value': results.p_value,\n",
    "                    'n': f\"{upper_n}/{lower_n}\"\n",
    "                }\n",
    "                \n",
    "                # Plot KM curves for this subtype\n",
    "                kmf = KaplanMeierFitter()\n",
    "                for group in ['Upper', 'Lower']:\n",
    "                    mask = (plot_data['expression_group'] == group)\n",
    "                    if mask.sum() > 0:\n",
    "                        kmf.fit(plot_data.loc[mask, 'survival_time'],\n",
    "                               plot_data.loc[mask, 'event'],\n",
    "                               label=f'{subtype}-{group}')\n",
    "                        kmf.plot(ax=ax, ci_show=False, \n",
    "                                color=colors[subtype][group],\n",
    "                                linestyle='-' if group == 'Upper' else '--')\n",
    "        \n",
    "        # Add stats in bottom right corner\n",
    "        stats_text = []\n",
    "        for subtype in sorted(stats_results.keys()):\n",
    "            p_val = stats_results[subtype]['p_value']\n",
    "            n = stats_results[subtype]['n']\n",
    "            stars = '***' if p_val <= 0.001 else '**' if p_val <= 0.01 else '*' if p_val <= 0.05 else 'ns'\n",
    "            stats_text.append(f'{subtype} (n={n}): {stars} p={p_val:.3f}')\n",
    "        \n",
    "        # Position stats text in bottom right\n",
    "        ax.text(0.95, 0.25, '\\n'.join(stats_text),\n",
    "                transform=ax.transAxes,\n",
    "                horizontalalignment='right',\n",
    "                verticalalignment='bottom',\n",
    "                fontsize=9,\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        # Customize plot\n",
    "        #ax.set_title(protein_name.split('(')[0].strip(), fontsize=10)\n",
    "        split_title = protein_name.replace(\" (\", \"\\n(\")\n",
    "        ax.set_title(split_title, fontsize=10)\n",
    "        if row == 1 or row == 0:  # Only bottom row\n",
    "            ax.set_xlabel('Time (months)')\n",
    "        if col == 0:  # Only leftmost column\n",
    "            ax.set_ylabel('Survival probability')\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{figure_path}/all_proteins_survival.png', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis\n",
    "create_multiplot_survival_analysis(\n",
    "    df=df,\n",
    "    protein_list=top_biomarkers,\n",
    "    protein_names=protein_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_names = ['CCDC80', 'MAPT', 'PRDX3', 'FKBP4', 'APEX1', 'FOSB', 'PAG1', 'PPP3R1', 'THOP1', 'WASF1'] \n",
    "\n",
    "def create_multiplot_survival_analysis(df, protein_list, protein_names):\n",
    "    # Set up the figure with GridSpec\n",
    "    fig = plt.figure(figsize=(22, 10))\n",
    "    gs = GridSpec(2, 5, figure=fig)\n",
    "    \n",
    "    colors = {\n",
    "        'Upper': '#1a9850',  # Dark green\n",
    "        'Middle': '#fee08b', # Yellow\n",
    "        'Lower': '#d73027'   # Dark red\n",
    "    }\n",
    "    \n",
    "    # Process each protein\n",
    "    for idx, (protein_id, protein_name) in enumerate(zip(protein_list, protein_names)):\n",
    "        row = idx // 5\n",
    "        col = idx % 5\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        # Calculate tertiles for the entire dataset\n",
    "        df['expression_group'] = pd.qcut(df[protein_id], \n",
    "                                       q=3, \n",
    "                                       labels=['Lower', 'Middle', 'Upper'])\n",
    "        \n",
    "        # Get group sizes\n",
    "        group_sizes = df['expression_group'].value_counts()\n",
    "        \n",
    "        # Perform logrank test\n",
    "        upper_data = df[df['expression_group'] == 'Upper']\n",
    "        lower_data = df[df['expression_group'] == 'Lower']\n",
    "        \n",
    "        results = logrank_test(\n",
    "            upper_data['survival_time'],\n",
    "            lower_data['survival_time'],\n",
    "            upper_data['event'],\n",
    "            lower_data['event']\n",
    "        )\n",
    "        \n",
    "        # Plot KM curves\n",
    "        kmf = KaplanMeierFitter()\n",
    "        for group in ['Upper', 'Middle', 'Lower']:\n",
    "            mask = (df['expression_group'] == group)\n",
    "            if mask.sum() > 0:\n",
    "                kmf.fit(df.loc[mask, 'survival_time'],\n",
    "                       df.loc[mask, 'event'],\n",
    "                       label=f'{group} (n={group_sizes[group]})')\n",
    "                kmf.plot(ax=ax, \n",
    "                        ci_show=False,\n",
    "                        color=colors[group],\n",
    "                        linestyle='-')\n",
    "        \n",
    "        # Add p-value\n",
    "        p_val = results.p_value\n",
    "        stars = '***' if p_val <= 0.001 else '**' if p_val <= 0.01 else '*' if p_val <= 0.05 else 'ns'\n",
    "        stats_text = f'p={p_val:.3e} {stars}'\n",
    "        \n",
    "        ax.text(0.95, 0.95, stats_text,\n",
    "                transform=ax.transAxes,\n",
    "                horizontalalignment='right',\n",
    "                verticalalignment='top',\n",
    "                fontsize=13,\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax.set_title(protein_name.split('(')[0].strip(), fontsize=16, fontweight='bold')\n",
    "        if row == 1 or row == 0:  # Only bottom row\n",
    "            ax.set_xlabel('Time (months)', fontsize=12)\n",
    "        if col == 0:  # Only leftmost column\n",
    "            ax.set_ylabel('Survival probability', fontsize=12)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        ax.legend(fontsize=13)  # Increase legend font size here\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{figure_path}/km_survival_tertiles.png', dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis\n",
    "create_multiplot_survival_analysis(\n",
    "    df=df,\n",
    "    protein_list=top_biomarkers,\n",
    "    protein_names=protein_names\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
